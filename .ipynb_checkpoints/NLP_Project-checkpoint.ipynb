{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project - Git Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron - Single Unit Neural Network\n",
    "Here I am simply going to try to construct from scratch and mostly from memory a perceptron using the tools given by PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-2-abc1a8a5df15>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-abc1a8a5df15>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "class Perceptron(nn.Module):\n",
    "    \n",
    "    # What is the __init__ method? It is necessary for all classes in python. It initializes an object of the class type.\n",
    "    # It is used to initialize an object of the type you are defining.\n",
    "    def __init__(self,input_dim):\n",
    "        super(Perceptron, self).__init()\n",
    "        self.fc1 = nn.Linear(input_dim, 1)\n",
    "    \n",
    "    # What is the forward method? The forward method defines a forward pass.\n",
    "    def forward():\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<enumerate at 0x1235e7280>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,4,100,12,44]\n",
    "enumerate(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 2), (2, 3), (3, 4), (4, 100), (5, 12), (6, 44)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in enumerate(a)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he',\n",
       " 'who',\n",
       " 'fights',\n",
       " 'with',\n",
       " 'monsters',\n",
       " 'might',\n",
       " 'take',\n",
       " 'care',\n",
       " 'lest',\n",
       " 'he',\n",
       " 'thereby',\n",
       " 'become',\n",
       " 'a',\n",
       " 'monster.',\n",
       " 'and',\n",
       " 'if',\n",
       " 'you',\n",
       " 'gaze',\n",
       " 'for',\n",
       " 'long',\n",
       " 'into',\n",
       " 'an',\n",
       " 'abyss,',\n",
       " 'the',\n",
       " 'abyss',\n",
       " 'gazes',\n",
       " 'also',\n",
       " 'into',\n",
       " 'you.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"He who fights with monsters might take care lest he thereby become a monster. And if you gaze for long into an abyss, the abyss gazes also into you.\"\n",
    "sentence = sentence.lower()\n",
    "words = sentence.split(' ')\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he',\n",
       " 'who',\n",
       " 'fights',\n",
       " 'with',\n",
       " 'monsters',\n",
       " 'might',\n",
       " 'take',\n",
       " 'care',\n",
       " 'lest',\n",
       " 'he',\n",
       " 'thereby',\n",
       " 'become',\n",
       " 'a',\n",
       " 'monster',\n",
       " 'and',\n",
       " 'if',\n",
       " 'you',\n",
       " 'gaze',\n",
       " 'for',\n",
       " 'long',\n",
       " 'into',\n",
       " 'an',\n",
       " 'abyss',\n",
       " 'the',\n",
       " 'abyss',\n",
       " 'gazes',\n",
       " 'also',\n",
       " 'into',\n",
       " 'you']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "for i in range(len(words)):\n",
    "    words[i] = words[i].strip(string.punctuation)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['if',\n",
       " 'take',\n",
       " 'an',\n",
       " 'also',\n",
       " 'become',\n",
       " 'monster',\n",
       " 'he',\n",
       " 'a',\n",
       " 'and',\n",
       " 'long',\n",
       " 'might',\n",
       " 'thereby',\n",
       " 'you',\n",
       " 'for',\n",
       " 'abyss',\n",
       " 'the',\n",
       " 'monsters',\n",
       " 'lest',\n",
       " 'fights',\n",
       " 'with',\n",
       " 'into',\n",
       " 'who',\n",
       " 'gaze',\n",
       " 'gazes',\n",
       " 'care']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(set(words))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'if': 0,\n",
       " 'take': 1,\n",
       " 'an': 2,\n",
       " 'also': 3,\n",
       " 'become': 4,\n",
       " 'monster': 5,\n",
       " 'he': 6,\n",
       " 'a': 7,\n",
       " 'and': 8,\n",
       " 'long': 9,\n",
       " 'might': 10,\n",
       " 'thereby': 11,\n",
       " 'you': 12,\n",
       " 'for': 13,\n",
       " 'abyss': 14,\n",
       " 'the': 15,\n",
       " 'monsters': 16,\n",
       " 'lest': 17,\n",
       " 'fights': 18,\n",
       " 'with': 19,\n",
       " 'into': 20,\n",
       " 'who': 21,\n",
       " 'gaze': 22,\n",
       " 'gazes': 23,\n",
       " 'care': 24}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index = {word: index for index, word in enumerate(vocab)}\n",
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "         0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create a one-hot encoding.\n",
    "import numpy as np\n",
    "import torch\n",
    "one_hot_encoding = []\n",
    "for word in vocab:\n",
    "    temp = np.zeros(vocab_size)\n",
    "    temp[word_to_index[word]] = 1\n",
    "    one_hot_encoding.append([int(i) for i in temp.tolist()])\n",
    "one_hot_encoding = torch.tensor(one_hot_encoding)\n",
    "one_hot_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.functional import one_hot\n",
    "\n",
    "word_indexes = torch.tensor([word_to_index[word] for word in vocab], dtype=torch.long)\n",
    "\n",
    "one_hot_encoding2 = one_hot(word_indexes)\n",
    "\n",
    "one_hot_encoding == one_hot_encoding2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "CONTEXT_SIZE = 2\n",
    "\n",
    "EMBEDDING_DIM = 10\n",
    "\n",
    "HIDDEN_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['he', 'who'], 'fights'),\n",
       " (['who', 'fights'], 'with'),\n",
       " (['fights', 'with'], 'monsters'),\n",
       " (['with', 'monsters'], 'might'),\n",
       " (['monsters', 'might'], 'take'),\n",
       " (['might', 'take'], 'care'),\n",
       " (['take', 'care'], 'lest'),\n",
       " (['care', 'lest'], 'he'),\n",
       " (['lest', 'he'], 'thereby'),\n",
       " (['he', 'thereby'], 'become'),\n",
       " (['thereby', 'become'], 'a'),\n",
       " (['become', 'a'], 'monster'),\n",
       " (['a', 'monster'], 'and'),\n",
       " (['monster', 'and'], 'if'),\n",
       " (['and', 'if'], 'you'),\n",
       " (['if', 'you'], 'gaze'),\n",
       " (['you', 'gaze'], 'for'),\n",
       " (['gaze', 'for'], 'long'),\n",
       " (['for', 'long'], 'into'),\n",
       " (['long', 'into'], 'an'),\n",
       " (['into', 'an'], 'abyss'),\n",
       " (['an', 'abyss'], 'the'),\n",
       " (['abyss', 'the'], 'abyss'),\n",
       " (['the', 'abyss'], 'gazes'),\n",
       " (['abyss', 'gazes'], 'also'),\n",
       " (['gazes', 'also'], 'into'),\n",
       " (['also', 'into'], 'you')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BUILDING TRIGRAMS\n",
    "trigrams = [([words[i-2], words[i-1]], words[i]) for i in range(2, len(words))]\n",
    "trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9812, -1.0526,  0.5018, -0.2590,  0.4134, -0.5234,  0.5717,  0.3450,\n",
       "         -1.1908, -0.1428],\n",
       "        [-0.8773, -0.0970,  0.4067,  0.5730, -0.4119,  0.1441, -1.2060, -0.0388,\n",
       "          0.2882,  2.2957],\n",
       "        [-0.9929,  1.1481,  1.1436, -0.0893,  1.0575,  0.4105,  0.6003, -2.2144,\n",
       "          0.2616, -0.4584],\n",
       "        [ 1.0645,  0.3377, -0.8523,  0.1223,  0.2922,  0.5967, -1.0821,  0.5128,\n",
       "          0.5001,  1.4056],\n",
       "        [-0.9627,  1.6055,  1.4089,  0.1682,  0.9918,  0.3598,  0.8756, -2.4481,\n",
       "          2.4634,  0.3930],\n",
       "        [ 1.3530,  2.1350,  0.5662, -1.2188, -1.0073, -1.0038,  0.0203, -1.4928,\n",
       "         -0.5251,  1.3103],\n",
       "        [ 0.9181,  0.9022, -1.0495,  1.0628, -0.0980, -0.1198,  0.0245, -0.7863,\n",
       "         -1.3578, -1.1098],\n",
       "        [ 0.1854, -1.4153, -0.6394,  2.0770,  0.9932, -0.9177,  0.0990, -3.0511,\n",
       "         -0.9700,  1.2766],\n",
       "        [ 0.5134,  1.0748,  0.2907,  0.2415, -0.9919, -0.8349,  0.3034, -0.1958,\n",
       "         -0.5878, -0.0625],\n",
       "        [-1.2100, -0.0576,  0.2026,  0.1430, -0.8486,  0.0304,  0.3173,  1.9685,\n",
       "          0.2507,  1.5423],\n",
       "        [-2.2465,  0.7000, -0.6296, -0.4932, -0.7266,  0.6619,  0.3773,  0.3105,\n",
       "          0.4234,  0.0968],\n",
       "        [-1.1570, -0.2475, -0.2835, -1.9886, -0.5304,  1.8264,  1.6092,  0.8508,\n",
       "          1.4680,  1.3821],\n",
       "        [ 0.1899,  0.1505, -0.1253,  0.2374, -1.1838, -0.1939, -1.6887,  1.5401,\n",
       "         -0.4621,  0.8692],\n",
       "        [-0.0443, -0.9828,  0.5171,  0.1801, -0.1055, -0.6797, -1.9617, -0.5186,\n",
       "          0.0995,  0.0434],\n",
       "        [-1.1844,  0.5733, -1.5011, -0.9255,  1.0311, -0.5466, -0.0899,  0.8767,\n",
       "          0.1079, -0.6902],\n",
       "        [-0.4586, -0.2781,  0.2233,  0.4909, -0.0773, -0.0287,  1.0276,  0.1043,\n",
       "          0.4045, -0.9948],\n",
       "        [ 0.7782,  0.2535, -0.8286,  2.2988, -0.6036,  0.8850, -0.0650, -0.0687,\n",
       "         -0.9225, -0.9584],\n",
       "        [ 0.6543,  0.5448,  0.0744,  0.4606, -1.7292, -0.6430,  0.3999, -0.8437,\n",
       "         -1.0381,  1.8900],\n",
       "        [ 2.4234,  0.2876, -0.6208, -1.2554, -0.2911, -0.3335, -0.2589, -0.4979,\n",
       "         -0.5843,  0.6391],\n",
       "        [ 0.0324, -0.2462, -0.7791, -0.0748,  0.1110,  0.3984,  0.0586,  1.5799,\n",
       "         -1.6263, -0.7928],\n",
       "        [ 1.7019, -1.1258, -0.3192, -0.9258,  0.1489,  1.0470,  0.4278, -0.5769,\n",
       "         -0.7279, -0.9671],\n",
       "        [-0.7787, -0.5057,  0.5276,  0.6341,  0.0544,  2.0301,  0.1390,  0.8651,\n",
       "          0.1187,  0.5569],\n",
       "        [-0.4052,  1.3608, -0.5685,  0.7227, -0.6204,  0.6071,  0.4866,  0.5250,\n",
       "         -0.1877,  1.5478],\n",
       "        [-0.1404, -0.1575,  0.5713,  0.2355,  0.8245,  0.7999,  0.2275, -1.3999,\n",
       "          2.6547,  0.7531],\n",
       "        [-1.1832, -1.9636, -0.5011, -0.2832,  1.1651, -0.5847,  0.0301,  1.7814,\n",
       "         -1.2188,  0.2608]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "embedding = nn.Embedding(vocab_size, EMBEDDING_DIM)\n",
    "embedding(torch.tensor([i for i in range(vocab_size)])).view((vocab_size, EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 8, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(5,7,8).stride()\n",
    "# To jump from the i_th dim_1 to the j_th dim_1 holding all else equal takes |j-i|*56 moves.\n",
    "# To jump from the i_th dim_2 to the j_th dim_2 holding all else equal takes |j-i|*8 moves.\n",
    "# To jump from the i_th dim_3 to the j_th dim_3 holding all else equal takes |j-i| moves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.nn.modules.module.Module,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embeddings are modified modules.\n",
    "nn.Embedding.__bases__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(object,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the fundamental PyTorch class on which all models are built.\n",
    "nn.Module.__bases__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore the following NGram Language Modeler...\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NGramLanguageModeler(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        # This use of super makes Python start looking for an __init__ method associated with a dummy object\n",
    "        # instantiation (self) one level above NGramLanguageModel (in this case Module).\n",
    "        super(NGramLanguageModeler, self).__init__() # Does the simple Module initialization.\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim) # This module now contains a sub-module of class Embeddings.\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, HIDDEN_DIM) # Again linear is a simple affine transformation.\n",
    "        self.linear2 = nn.Linear(HIDDEN_DIM, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs): # Takes a temporary object of this class (to access data and methods) and also takes input vector\n",
    "        # The forward pass.\n",
    "        # We have a model structure, composed essentially of a collection of matrix multiplications\n",
    "        # and other similar computations which keep track of their source so that gradients can be computed.\n",
    "        # With the forward pass, we simply run our computations in order.\n",
    "        embeds = self.embeddings(inputs).view(1,-1) # We need to essentially concatenate these two vectors\n",
    "        out = F.relu(self.linear1(embeds)) \n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out) # gives us a log probability over the vocabulary\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically above we have our model. But it is trash. It doesn't give good results because all of the weights and biases and embeddings are totally random. We need to hone it by giving it some feedback so that it adjusts itself. We do this by running it on some data and computing the loss with respect to some target output, and then backpropagating. We do this for several epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "learning_rate = 0.001\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss() # negative log likelihood\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of NGramLanguageModeler(\n",
       "  (embeddings): Embedding(25, 10)\n",
       "  (linear1): Linear(in_features=20, out_features=256, bias=True)\n",
       "  (linear2): Linear(in_features=256, out_features=25, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 10])\n",
      "torch.Size([256, 20])\n",
      "torch.Size([256])\n",
      "torch.Size([25, 256])\n",
      "torch.Size([25])\n"
     ]
    }
   ],
   "source": [
    "# Let's briefly inspect the model parameters\n",
    "# We can use the iterator model.parameters() to get the tensors which constitute our models parameters.\n",
    "# In this case, the output reveals that the first parameter is a matrix with 25 rows and ten columns.\n",
    "# That is our embedding table of 25 words each with a vector representation of length 10.\n",
    "# Our next paramater is a matrix with 256 rows and 20 columns. This represents weights of the connections between\n",
    "# our two stacked word embeddings (from the conditioned word pair) and our 256 hidden units.\n",
    "# The next is a vector of length 256. These are the biases of our 256 hidden units.\n",
    "# Then we have a matrix with 25 rows and 256 columns. These are the weights from the 256 hidden units\n",
    "# to the 25 possible output words.\n",
    "for param in model.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/27 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'word_to_ix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-de8252a13ef2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# For each context, for each of the two words in the context, replace it with its index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mcontext_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_to_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Zero out gradients since gradients are accumulated in PyTorch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-de8252a13ef2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# For each context, for each of the two words in the context, replace it with its index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mcontext_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_to_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Zero out gradients since gradients are accumulated in PyTorch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_to_ix' is not defined"
     ]
    }
   ],
   "source": [
    "# Now let's train our model.\n",
    "from tqdm import tqdm # A really great progress bar library\n",
    "\n",
    "for epoch in range(25):\n",
    "    total_loss = 0 # Initialize loss to 0\n",
    "    \n",
    "    iterator = tqdm(trigrams)\n",
    "    # tqdm wraps an iterable and returns progress by showing how many iterations you've gone through\n",
    "    # and how many left you have to go. tqdm(iterable) can be treated like an iterator.\n",
    "    \n",
    "    for context, target in iterator:\n",
    "        # For each context, for each of the two words in the context, replace it with its index.\n",
    "        context_ids = torch.tensor([word_to_ix[w] for w in context], dtype=torch.long)\n",
    "        \n",
    "        # Zero out gradients since gradients are accumulated in PyTorch.\n",
    "        model.zero_grad()\n",
    "        \n",
    "        log_probs = model(context_ids)\n",
    "        \n",
    "        # previously defined as negative log likelihood loss\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[target]]), dtype=torch.long)\n",
    "        \n",
    "        # This computes the gradient of the loss with respect to ALL THE PARAMETERS\n",
    "        # through the efficient dynamic programming algorithm known as backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Here the optimizer takes one step (with size of learning rate) in the direction of the gradient.\n",
    "        # In other words it updates all parameters (considered as a vector) by some scaled version of the \n",
    "        # gradient vector.\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/27 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tqdm.std.tqdm"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
